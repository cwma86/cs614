{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de2ae45",
   "metadata": {},
   "source": [
    "### Mauer Cory\n",
    "# CS 614 Assignment 4: Gaming\n",
    "## Black Jack card counting agent\n",
    "\n",
    "## Pitch: \n",
    "For this project I chose to build a gaming agent to play blackjack. While most other electronic black jack games are implemented using randomly selected cards, I wanted to model an actual deck of cards to see if I could train a model to count cards using the extra state. The goal of this agent is increase odds of winning in blackjack which could be implemented into a competative online blackjack game. \n",
    "\n",
    "## Data source:\n",
    "The data source of this project is a black jack game that I implemented my self. At a high level, there is one modeled deck of cards and cards are handed out to a player and dealer until our deck reaches 17 cards at which point the \"deck\" is shuffled. the API into the black jack games provides users and models with the following state \n",
    "** players sum of card\n",
    "** dealers sum of cards\n",
    "** if the player has an ace\n",
    "** an array showing the number of cards we have seen 1, 2, ... , 10\n",
    "\n",
    "## Model and data justification:\n",
    "For building my agent I decided to leverage a deep q model via tensor flow. Since each move of black jack can effectively be treated as a unique hand/game there was no need to model any temporal aspects which led to this relatively simple model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d22c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                480       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,962\n",
      "Trainable params: 4,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.optimizers import Adam\n",
    "    import tensorflow as tf\n",
    "    state_size = 14\n",
    "    action_state = 2\n",
    "    lr = 0.001\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(action_state, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04632e1d-40d7-4b8c-9ddd-72b52c9c2000",
   "metadata": {},
   "source": [
    "## Commented examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973067b0-2401-4cd3-abba-b9d21e264c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game State:\n",
      "player sum: 12 isAcePresent: 0\n",
      "dealer card value: 2\n",
      "cards played: \n",
      " ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\n",
      "  [0, 1, 1, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from black_jack import BlackJack\n",
    "from black_jack_deepq import DQNAgent\n",
    "\n",
    "# Initialize the game agent with the trained model\n",
    "state_size = 13  # Player's sum, dealer's showing card, and player has a usable ace\n",
    "action_size = 2  # Stay (0) or Hit (1)\n",
    "batch_size = 128 # number of steps to be used for training\n",
    "agent = DQNAgent(state_size, action_size, batch_size, model=\"./black_jack_model\", verbose=False)\n",
    "\n",
    "# start a new game of black_jack\n",
    "env = BlackJack(cardState=True)\n",
    "state, _ = env.reset() # Reset starts a new game and returns the state\n",
    "print(f\"Game State:\")\n",
    "print(f\"player sum: {state[0]} isAcePresent: {state[3]}\")\n",
    "print(f\"dealer card value: {state[1]}\")\n",
    "print(f\"cards played: \\n ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\")\n",
    "print(f\"  {state[3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c368a3a-7bf8-44e4-ad26-0b5b45d39f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action : 0- stay, 1- hit \n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "# Get the agents move\n",
    "action = agent.act(state)\n",
    "print(f\"action : 0- stay, 1- hit \\n {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcd93980-5cba-44af-b0b1-31f2fade8be2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: -1 = loss, 0.0 = not finished/draw, 1 = win 1.0\n",
      " 1.0\n",
      "final state [12, 26, False, 0, 1, 1, 1, 0, 0, 0, 0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _1, _2 = env.step(action)\n",
    "print(f\"reward: -1 = loss, 0.0 = not finished/draw, 1 = win {reward}\")\n",
    "print(f\" {reward}\")\n",
    "print(f\"final state {next_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292825d-eb58-49cb-806c-e7d3ea7b1f7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this example the agent made what I would consider to be a not ideal selection by staying on a 12. However the agent ended up winning the had due to the dealer busting on a hit. Since the model was trained to obsever the state of the seen cards from a single deck, perhaps this was a good choice given that there was a 16/52 probability in receiving a 10 card on a hit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2ac29",
   "metadata": {},
   "source": [
    "## Testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ee1ec8c2-a8df-42ff-bae8-a21625ed1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the trained model for 1000 games\n",
      "Win count 293 number of games1000\n",
      "Win rate 29.299999999999997%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "state_size = 13  # Player's sum, dealer's showing card, and player has a usable ace\n",
    "number_of_games = 1000\n",
    "# Play the game with the trained agent\n",
    "win_count = 0\n",
    "print(f\"testing the trained model for {number_of_games} games\")\n",
    "for i in range(number_of_games):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _1, _2 = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        state = next_state\n",
    "        if reward >= 1.0:\n",
    "            win_count = win_count + 1\n",
    "        # env.render()\n",
    "\n",
    "print(f\"Win count {win_count} number of games{number_of_games}\")\n",
    "print(f\"Win rate {(win_count/number_of_games)*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09424a46-b368-4179-9127-48613be8fa75",
   "metadata": {},
   "source": [
    "The model does not currently preform as well as I expected, however this is likely caused from a relative lack of available training time. Given more time and compute resources I beleive this model could acieve closer to the statistic "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427480f",
   "metadata": {},
   "source": [
    "## Code and run Instructions\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da81b31",
   "metadata": {},
   "source": [
    "### I agree to sharing this assignment with other students in the course after grading has occurred. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
