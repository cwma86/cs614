{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2de2ae45",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Mauer Cory\n",
    "# CS 614 Assignment 4: Gaming\n",
    "## Black Jack card counting agent\n",
    "\n",
    "## Pitch: \n",
    "For this project I chose to build a gaming agent to play blackjack. While most other electronic black jack games are implemented using randomly selected cards, I wanted to model an actual deck of cards to see if I could train a model to \"count cards\" using the extra state. My goal is to seek out investment to aquire capital to develop a high quality black jack agent that can be used to create a online blackjack game with high dealer win probabilities. \n",
    "\n",
    "## Data source:\n",
    "The data source of this project is a black jack game that I implemented my self. At a high level, there is one modeled deck of cards and cards are handed out to a player and dealer until our deck reaches 17 cards at which point the \"deck\" is shuffled. the API into the black jack games provides users and models with the following state\n",
    "\n",
    "* players sum of card\n",
    "* dealers sum of cards\n",
    "* if the player has an ace\n",
    "* an array showing the number of cards we have seen 1, 2, ... , 10\n",
    "\n",
    "## Model and data justification:\n",
    "For building my agent I decided to leverage a deep q model via tensor flow. Since each move of black jack can effectively be treated as a unique hand/game there was no need to model any temporal aspects which led to this relatively simple model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d22c7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                480       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               4224      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,962\n",
      "Trainable params: 4,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    import os\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.optimizers import Adam\n",
    "    import tensorflow as tf\n",
    "    state_size = 14\n",
    "    action_state = 2\n",
    "    lr = 0.001\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(action_state, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr))\n",
    "\n",
    "    model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04632e1d-40d7-4b8c-9ddd-72b52c9c2000",
   "metadata": {},
   "source": [
    "## Commented examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce1c0b93-d2bd-4ac4-9947-f0e36854af9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "from black_jack import BlackJack\n",
    "from black_jack_deepq import DQNAgent\n",
    "\n",
    "# Initialize the game agent with the trained model\n",
    "state_size = 13  # Player's sum, dealer's showing card, and player has a usable ace\n",
    "action_size = 2  # Stay (0) or Hit (1)\n",
    "batch_size = 128 # number of steps to be used for training\n",
    "agent = DQNAgent(state_size, action_size, batch_size, model=\"./black_jack_model_test\", verbose=False)\n",
    "\n",
    "# start a new game of black_jack\n",
    "env = BlackJack(cardState=True)\n",
    "state, _ = env.reset() # Reset starts a new game and returns the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b83b44fe-1c25-4c9e-ac82-90709020ddf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game State:\n",
      "player sum: 10 isAcePresent: False\n",
      "dealer card value: 10\n",
      "cards played: \n",
      " ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\n",
      "  [0, 0, 0, 0, 2, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Game State:\")\n",
    "print(f\"player sum: {state[0]} isAcePresent: {state[2]}\")\n",
    "print(f\"dealer card value: {state[1]}\")\n",
    "print(f\"cards played: \\n ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\")\n",
    "print(f\"  {state[3:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7d6000f-e3cd-4cff-8727-e83a209a0779",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action : 0- stay, 1- hit \n",
      " 1\n",
      "Game State:\n",
      "player sum: 16 isAcePresent: False\n",
      "dealer card value: 10\n",
      "cards played: \n",
      " ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\n",
      "  [0, 0, 0, 0, 2, 1, 0, 0, 0, 1]\n",
      "reward: 0.0\n",
      "action : 0- stay, 1- hit \n",
      " 1\n",
      "Game State:\n",
      "player sum: 18 isAcePresent: False\n",
      "dealer card value: 10\n",
      "cards played: \n",
      " ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\n",
      "  [0, 1, 0, 0, 2, 1, 0, 0, 0, 1]\n",
      "reward: 0.0\n",
      "action : 0- stay, 1- hit \n",
      " 0\n",
      "Game State:\n",
      "player sum: 18 isAcePresent: False\n",
      "dealer card value: 24\n",
      "cards played: \n",
      " ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\n",
      "  [0, 1, 0, 1, 2, 1, 0, 0, 0, 2]\n",
      "reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    # Get the agents move\n",
    "    action = agent.act(state)\n",
    "    print(f\"action : 0- stay, 1- hit \\n {action}\")\n",
    "    next_state, reward, done, _1, _2 = env.step(action)\n",
    "    print(f\"Game State:\")\n",
    "    print(f\"player sum: {next_state[0]} isAcePresent: {next_state[2]}\")\n",
    "    print(f\"dealer card value: {next_state[1]}\")\n",
    "    print(f\"cards played: \\n ace, 2, 3, 4, 5, 6, 7, 8, 9, 10 (10, jack, queen, king)\")\n",
    "    print(f\"  {next_state[3:]}\")\n",
    "    print(f\"reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292825d-eb58-49cb-806c-e7d3ea7b1f7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this game the agent behaved similar to the dealer logic by hitting on 16. Personally I am not sure I would have made the same choice given the fact that the dealer showed a 10 it high a high probability of busting if they hit. However looking at the seen cards, this  may have been the correct choice given that there was a 18/48 probability that the card drawn would be below our \"bust\" threshold of 22. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a2ac29",
   "metadata": {},
   "source": [
    "## Testing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee1ec8c2-a8df-42ff-bae8-a21625ed1c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing the trained model for 1000 games\n",
      "Win count 291 number of games1000\n",
      "Win rate 29.099999999999998%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "state_size = 13  # Player's sum, dealer's showing card, and player has a usable ace\n",
    "number_of_games = 1000\n",
    "# Play the game with the trained agent\n",
    "win_count = 0\n",
    "print(f\"testing the trained model for {number_of_games} games\")\n",
    "for i in range(number_of_games):\n",
    "    state, _ = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _1, _2 = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        state = next_state\n",
    "        if reward >= 1.0:\n",
    "            win_count = win_count + 1\n",
    "        # env.render()\n",
    "\n",
    "print(f\"Win count {win_count} number of games{number_of_games}\")\n",
    "print(f\"Win rate {(win_count/number_of_games)*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09424a46-b368-4179-9127-48613be8fa75",
   "metadata": {},
   "source": [
    "The model does not currently preform as well as I expected, however this is likely caused from a relative lack of available training time. Given more time and compute resources I beleive this model could acieve closer to the statistic average win rate of approximately 44%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e427480f",
   "metadata": {},
   "source": [
    "## Code and run Instructions\n",
    "Code can be found here (https://github.com/cwma86/cs614/tree/main/gaming). Start with the `README.md` which will provide install and setup direction on creating a python virtual environement. Once setup you can play a game of command line black jack using `black_jack.py` at the end of each game you will be given statistics on how well you did. After that you can attempt to train your own black jack agent using the deepq learning model in `black_jack_deepq.py`. Instructions for both can be found in the `README.md` or use the `--help` menu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da81b31",
   "metadata": {},
   "source": [
    "### I agree to sharing this assignment with other students in the course after grading has occurred. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9b057-6a70-4712-928a-3e19a24bf048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
